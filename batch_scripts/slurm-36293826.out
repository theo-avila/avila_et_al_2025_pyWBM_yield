2025-04-06 01:24:39,675 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.1.199:42267'
2025-04-06 01:24:41,570 - distributed.worker - INFO -       Start worker at:     tcp://10.6.1.199:45995
2025-04-06 01:24:41,570 - distributed.worker - INFO -          Listening to:     tcp://10.6.1.199:45995
2025-04-06 01:24:41,570 - distributed.worker - INFO -           Worker name:             SLURMCluster-1
2025-04-06 01:24:41,570 - distributed.worker - INFO -          dashboard at:           10.6.1.199:43357
2025-04-06 01:24:41,570 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.8.79:46415
2025-04-06 01:24:41,570 - distributed.worker - INFO - -------------------------------------------------
2025-04-06 01:24:41,570 - distributed.worker - INFO -               Threads:                          2
2025-04-06 01:24:41,570 - distributed.worker - INFO -                Memory:                 200.00 GiB
2025-04-06 01:24:41,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jo99eq5v
2025-04-06 01:24:41,571 - distributed.worker - INFO - -------------------------------------------------
2025-04-06 01:24:42,673 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-04-06 01:24:42,674 - distributed.worker - INFO -         Registered to:      tcp://10.6.8.79:46415
2025-04-06 01:24:42,674 - distributed.worker - INFO - -------------------------------------------------
2025-04-06 01:24:42,674 - distributed.core - INFO - Starting established connection to tcp://10.6.8.79:46415
2025-04-06 01:24:46,754 - distributed.nanny - INFO - Worker process 1042868 was killed by signal 11
2025-04-06 01:24:47,305 - distributed.nanny - WARNING - Restarting worker
2025-04-06 01:24:48,974 - distributed.worker - INFO -       Start worker at:     tcp://10.6.1.199:38799
2025-04-06 01:24:48,974 - distributed.worker - INFO -          Listening to:     tcp://10.6.1.199:38799
2025-04-06 01:24:48,974 - distributed.worker - INFO -           Worker name:             SLURMCluster-1
2025-04-06 01:24:48,974 - distributed.worker - INFO -          dashboard at:           10.6.1.199:43109
2025-04-06 01:24:48,974 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.8.79:46415
2025-04-06 01:24:48,974 - distributed.worker - INFO - -------------------------------------------------
2025-04-06 01:24:48,975 - distributed.worker - INFO -               Threads:                          2
2025-04-06 01:24:48,975 - distributed.worker - INFO -                Memory:                 200.00 GiB
2025-04-06 01:24:48,975 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j71_uhqa
2025-04-06 01:24:48,975 - distributed.worker - INFO - -------------------------------------------------
2025-04-06 01:24:49,496 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-04-06 01:24:49,497 - distributed.worker - INFO -         Registered to:      tcp://10.6.8.79:46415
2025-04-06 01:24:49,497 - distributed.worker - INFO - -------------------------------------------------
2025-04-06 01:24:49,497 - distributed.core - INFO - Starting established connection to tcp://10.6.8.79:46415
/storage/home/cta5244/work/avila_et_al_2025_pyWBM_yield/6_future_processing.py:154: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.
  ds_soilpyWBM_initial['time'] = ds_soilpyWBM_initial.indexes['time'].to_datetimeindex()
slurmstepd: error: *** JOB 36293826 ON p-sc-2429 CANCELLED AT 2025-04-06T01:45:47 ***
2025-04-06 01:45:47,507 - distributed.worker - INFO - Stopping worker at tcp://10.6.1.199:38799. Reason: scheduler-close
