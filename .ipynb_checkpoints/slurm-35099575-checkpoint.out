2025-03-14 22:40:23,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.8.13:35665'
2025-03-14 22:40:24,708 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-kstpuk5n', purging
2025-03-14 22:40:24,798 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-2h5g7w3g', purging
2025-03-14 22:40:24,854 - distributed.diskutils - ERROR - Failed to remove '/scratch/cta5244/dask-workers/dask-scratch-space/worker-2h5g7w3g' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/scratch/cta5244/dask-workers/dask-scratch-space/worker-2h5g7w3g'
2025-03-14 22:40:24,859 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-dh7_ji4u', purging
2025-03-14 22:40:24,923 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-pi3arv2b', purging
2025-03-14 22:40:24,996 - distributed.diskutils - ERROR - Failed to remove '/scratch/cta5244/dask-workers/dask-scratch-space/worker-pi3arv2b' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/scratch/cta5244/dask-workers/dask-scratch-space/worker-pi3arv2b'
2025-03-14 22:40:26,472 - distributed.worker - INFO -       Start worker at:      tcp://10.6.8.13:34523
2025-03-14 22:40:26,472 - distributed.worker - INFO -          Listening to:      tcp://10.6.8.13:34523
2025-03-14 22:40:26,472 - distributed.worker - INFO -           Worker name:             SLURMCluster-6
2025-03-14 22:40:26,472 - distributed.worker - INFO -          dashboard at:            10.6.8.13:43821
2025-03-14 22:40:26,472 - distributed.worker - INFO - Waiting to connect to: tcp://146.186.150.11:36245
2025-03-14 22:40:26,472 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:40:26,472 - distributed.worker - INFO -               Threads:                          1
2025-03-14 22:40:26,473 - distributed.worker - INFO -                Memory:                   4.00 GiB
2025-03-14 22:40:26,473 - distributed.worker - INFO -       Local Directory: /scratch/cta5244/dask-workers/dask-scratch-space/worker-g1_52o2k
2025-03-14 22:40:26,473 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:40:27,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-14 22:40:27,284 - distributed.worker - INFO -         Registered to: tcp://146.186.150.11:36245
2025-03-14 22:40:27,284 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:40:27,284 - distributed.core - INFO - Starting established connection to tcp://146.186.150.11:36245
2025-03-14 22:41:03,188 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.8.41:45813
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2075, in gather_dep
    response = await get_data_from_worker(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2878, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/core.py", line 1539, in connect
    return connect_attempt.result()
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.8.13:53504 remote=tcp://10.6.8.41:45813>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-03-14 22:41:06,167 - distributed.nanny - INFO - Worker process 4021467 was killed by signal 11
2025-03-14 22:41:06,187 - distributed.nanny - WARNING - Restarting worker
2025-03-14 22:41:07,241 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-qno_9u4e', purging
2025-03-14 22:41:08,129 - distributed.worker - INFO -       Start worker at:      tcp://10.6.8.13:35247
2025-03-14 22:41:08,129 - distributed.worker - INFO -          Listening to:      tcp://10.6.8.13:35247
2025-03-14 22:41:08,129 - distributed.worker - INFO -           Worker name:             SLURMCluster-6
2025-03-14 22:41:08,129 - distributed.worker - INFO -          dashboard at:            10.6.8.13:37981
2025-03-14 22:41:08,129 - distributed.worker - INFO - Waiting to connect to: tcp://146.186.150.11:36245
2025-03-14 22:41:08,129 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:08,129 - distributed.worker - INFO -               Threads:                          1
2025-03-14 22:41:08,129 - distributed.worker - INFO -                Memory:                   4.00 GiB
2025-03-14 22:41:08,129 - distributed.worker - INFO -       Local Directory: /scratch/cta5244/dask-workers/dask-scratch-space/worker-lwh96svp
2025-03-14 22:41:08,130 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:08,711 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-14 22:41:08,712 - distributed.worker - INFO -         Registered to: tcp://146.186.150.11:36245
2025-03-14 22:41:08,712 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:08,712 - distributed.core - INFO - Starting established connection to tcp://146.186.150.11:36245
2025-03-14 22:41:12,446 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.8.33:33605
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2075, in gather_dep
    response = await get_data_from_worker(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2881, in get_data_from_worker
    response = await send_recv(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.8.13:42682 remote=tcp://10.6.8.33:33605>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-03-14 22:41:12,458 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.8.41:37201
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2075, in gather_dep
    response = await get_data_from_worker(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2881, in get_data_from_worker
    response = await send_recv(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.8.13:57784 remote=tcp://10.6.8.41:37201>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-03-14 22:41:15,661 - distributed.worker - ERROR - Compute Failed
Key:       ('concatenate-mean_chunk-367a6debf57fcbe6c2f701701320497d', 15, 0, 0)
State:     executing
Task:  <Task ('concatenate-mean_chunk-367a6debf57fcbe6c2f701701320497d', 15, 0, 0) _execute_subgraph(...)>
Exception: "OSError(-101, 'NetCDF: HDF error')"
Traceback: '  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/dask/array/core.py", line 133, in getter\n    c = np.asarray(c)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 578, in __array__\n    return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 583, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 794, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 664, in get_duck_array\n    array = array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 657, in get_duck_array\n    array = self.array[self.key]\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 103, in __getitem__\n    return indexing.explicit_indexing_adapter(\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 1018, in explicit_indexing_adapter\n    result = raw_indexing_method(raw_key.tuple)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 115, in _getitem\n    original_array = self.get_array(needs_lock=False)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 94, in get_array\n    ds = self.datastore._acquire(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 455, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/contextlib.py", line 135, in __enter__\n    return next(self.gen)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n  File "src/netCDF4/_netCDF4.pyx", line 2521, in netCDF4._netCDF4.Dataset.__init__\n  File "src/netCDF4/_netCDF4.pyx", line 2158, in netCDF4._netCDF4._ensure_nc_success\n'

Exception ignored in: <function CachingFileManager.__del__ at 0x15522c5527a0>
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 250, in __del__
    self.close(needs_lock=False)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 234, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2669, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2636, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2164, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: HDF error
Type = File(72057594037927976) name='/'Type = File(72057594037927979) name='/'Type = File(72057594037927983) name='/'Type = File(72057594037927989) name='/'Type = File(72057594037928005) name='/'Type = File(72057594037928016) name='/'Type = File(72057594037928024) name='/'Type = File(72057594037928025) name='/'Type = File(72057594037928028) name='/'Type = File(72057594037928029) name='/'Type = File(72057594037928032) name='/'Type = File(72057594037928033) name='/'Type = File(72057594037928037) name='/'Type = File(72057594037928038) name='/'Type = File(72057594037928041) name='/'Type = File(72057594037928042) name='/'Type = File(72057594037928045) name='/'Type = File(72057594037928047) name='/'Type = File(72057594037928048) name='/'Type = File(72057594037928050) name='/'Type = File(72057594037928052) name='/'Type = File(72057594037928054) name='/'Type = File(72057594037928056) name='/'Type = File(72057594037928058) name='/'Type = File(72057594037928059) name='/'Type = File(72057594037928060) name='/'Type = File(72057594037928061) name='/'Type = File(72057594037928062) name='/'Type = File(72057594037928063) name='/'Type = File(72057594037928064) name='/'Type = File(72057594037928065) name='/'Type = File(72057594037928066) name='/'Type = File(72057594037928067) name='/'Type = File(72057594037928068) name='/'Type = File(72057594037928069) name='/'Type = File(72057594037928070) name='/'Type = File(72057594037928075) name='/'Type = File(72057594037928077) name='/'Type = File(72057594037928079) name='/'Type = File(72057594037928083) name='/'Type = File(72057594037928084) name='/'Type = File(72057594037928088) name='/'2025-03-14 22:41:27,119 - distributed.nanny - INFO - Worker process 4022741 was killed by signal 11
2025-03-14 22:41:27,123 - distributed.nanny - WARNING - Restarting worker
2025-03-14 22:41:28,111 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-4_u6x1re', purging
2025-03-14 22:41:28,965 - distributed.worker - INFO -       Start worker at:      tcp://10.6.8.13:40901
2025-03-14 22:41:28,965 - distributed.worker - INFO -          Listening to:      tcp://10.6.8.13:40901
2025-03-14 22:41:28,965 - distributed.worker - INFO -           Worker name:             SLURMCluster-6
2025-03-14 22:41:28,965 - distributed.worker - INFO -          dashboard at:            10.6.8.13:39439
2025-03-14 22:41:28,965 - distributed.worker - INFO - Waiting to connect to: tcp://146.186.150.11:36245
2025-03-14 22:41:28,965 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:28,965 - distributed.worker - INFO -               Threads:                          1
2025-03-14 22:41:28,965 - distributed.worker - INFO -                Memory:                   4.00 GiB
2025-03-14 22:41:28,965 - distributed.worker - INFO -       Local Directory: /scratch/cta5244/dask-workers/dask-scratch-space/worker-b3dye6u9
2025-03-14 22:41:28,966 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:29,451 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-14 22:41:29,452 - distributed.worker - INFO -         Registered to: tcp://146.186.150.11:36245
2025-03-14 22:41:29,452 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:29,452 - distributed.core - INFO - Starting established connection to tcp://146.186.150.11:36245
2025-03-14 22:41:35,796 - distributed.nanny - INFO - Worker process 4023710 was killed by signal 11
2025-03-14 22:41:35,799 - distributed.nanny - WARNING - Restarting worker
2025-03-14 22:41:36,695 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-3fi7i_b1', purging
2025-03-14 22:41:37,464 - distributed.worker - INFO -       Start worker at:      tcp://10.6.8.13:36373
2025-03-14 22:41:37,464 - distributed.worker - INFO -          Listening to:      tcp://10.6.8.13:36373
2025-03-14 22:41:37,464 - distributed.worker - INFO -           Worker name:             SLURMCluster-6
2025-03-14 22:41:37,464 - distributed.worker - INFO -          dashboard at:            10.6.8.13:45095
2025-03-14 22:41:37,464 - distributed.worker - INFO - Waiting to connect to: tcp://146.186.150.11:36245
2025-03-14 22:41:37,464 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:37,464 - distributed.worker - INFO -               Threads:                          1
2025-03-14 22:41:37,464 - distributed.worker - INFO -                Memory:                   4.00 GiB
2025-03-14 22:41:37,464 - distributed.worker - INFO -       Local Directory: /scratch/cta5244/dask-workers/dask-scratch-space/worker-yv9w550n
2025-03-14 22:41:37,465 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:38,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-14 22:41:38,075 - distributed.worker - INFO -         Registered to: tcp://146.186.150.11:36245
2025-03-14 22:41:38,075 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:41:38,075 - distributed.core - INFO - Starting established connection to tcp://146.186.150.11:36245
Exception ignored in: <function CachingFileManager.__del__ at 0x14e7128fc040>
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 250, in __del__
    self.close(needs_lock=False)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 234, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2669, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2636, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2164, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: HDF error
2025-03-14 22:41:40,640 - distributed.worker - ERROR - Compute Failed
Key:       ('concatenate-mean_chunk-ac76e1ae19c9dbd3466d95f7b0880f35', 2, 0, 0)
State:     executing
Task:  <Task ('concatenate-mean_chunk-ac76e1ae19c9dbd3466d95f7b0880f35', 2, 0, 0) _execute_subgraph(...)>
Exception: "OSError(-101, 'NetCDF: HDF error')"
Traceback: '  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/dask/array/core.py", line 133, in getter\n    c = np.asarray(c)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 578, in __array__\n    return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 583, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 794, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 664, in get_duck_array\n    array = array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 657, in get_duck_array\n    array = self.array[self.key]\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 103, in __getitem__\n    return indexing.explicit_indexing_adapter(\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 1018, in explicit_indexing_adapter\n    result = raw_indexing_method(raw_key.tuple)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 115, in _getitem\n    original_array = self.get_array(needs_lock=False)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 94, in get_array\n    ds = self.datastore._acquire(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 455, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/contextlib.py", line 135, in __enter__\n    return next(self.gen)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n  File "src/netCDF4/_netCDF4.pyx", line 2521, in netCDF4._netCDF4.Dataset.__init__\n  File "src/netCDF4/_netCDF4.pyx", line 2158, in netCDF4._netCDF4._ensure_nc_success\n'

2025-03-14 22:41:42,054 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.8.44:37201
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2075, in gather_dep
    response = await get_data_from_worker(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2881, in get_data_from_worker
    response = await send_recv(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.8.13:51626 remote=tcp://10.6.8.44:37201>: ConnectionResetError: [Errno 104] Connection reset by peer
Exception ignored in: <function CachingFileManager.__del__ at 0x14e7128fc040>
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 250, in __del__
    self.close(needs_lock=False)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 234, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2669, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2636, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2164, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: HDF error
2025-03-14 22:42:02,889 - distributed.worker - ERROR - Compute Failed
Key:       ('concatenate-mean_chunk-04a44b46693c9d63be39e3fb01dafe53', 14, 0, 0)
State:     executing
Task:  <Task ('concatenate-mean_chunk-04a44b46693c9d63be39e3fb01dafe53', 14, 0, 0) _execute_subgraph(...)>
Exception: "OSError(-101, 'NetCDF: HDF error')"
Traceback: '  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/dask/array/core.py", line 133, in getter\n    c = np.asarray(c)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 578, in __array__\n    return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 583, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 794, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 664, in get_duck_array\n    array = array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 657, in get_duck_array\n    array = self.array[self.key]\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 103, in __getitem__\n    return indexing.explicit_indexing_adapter(\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 1018, in explicit_indexing_adapter\n    result = raw_indexing_method(raw_key.tuple)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 115, in _getitem\n    original_array = self.get_array(needs_lock=False)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 94, in get_array\n    ds = self.datastore._acquire(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 455, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/contextlib.py", line 135, in __enter__\n    return next(self.gen)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n  File "src/netCDF4/_netCDF4.pyx", line 2521, in netCDF4._netCDF4.Dataset.__init__\n  File "src/netCDF4/_netCDF4.pyx", line 2158, in netCDF4._netCDF4._ensure_nc_success\n'

2025-03-14 22:42:05,689 - distributed.nanny - INFO - Worker process 4023774 was killed by signal 11
2025-03-14 22:42:05,699 - distributed.nanny - WARNING - Restarting worker
2025-03-14 22:42:06,781 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-eq7d67fn', purging
2025-03-14 22:42:07,684 - distributed.worker - INFO -       Start worker at:      tcp://10.6.8.13:44753
2025-03-14 22:42:07,685 - distributed.worker - INFO -          Listening to:      tcp://10.6.8.13:44753
2025-03-14 22:42:07,685 - distributed.worker - INFO -           Worker name:             SLURMCluster-6
2025-03-14 22:42:07,685 - distributed.worker - INFO -          dashboard at:            10.6.8.13:38613
2025-03-14 22:42:07,685 - distributed.worker - INFO - Waiting to connect to: tcp://146.186.150.11:36245
2025-03-14 22:42:07,685 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:07,685 - distributed.worker - INFO -               Threads:                          1
2025-03-14 22:42:07,685 - distributed.worker - INFO -                Memory:                   4.00 GiB
2025-03-14 22:42:07,685 - distributed.worker - INFO -       Local Directory: /scratch/cta5244/dask-workers/dask-scratch-space/worker-ix4r_j76
2025-03-14 22:42:07,685 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:08,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-14 22:42:08,399 - distributed.worker - INFO -         Registered to: tcp://146.186.150.11:36245
2025-03-14 22:42:08,399 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:08,400 - distributed.core - INFO - Starting established connection to tcp://146.186.150.11:36245
Exception ignored in: <function CachingFileManager.__del__ at 0x14b90596fb50>
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 250, in __del__
    self.close(needs_lock=False)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 234, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2669, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2636, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2164, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: HDF error
Exception ignored in: <function CachingFileManager.__del__ at 0x14b90596fb50>
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 250, in __del__
    self.close(needs_lock=False)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 234, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2669, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2636, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2164, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: HDF error
2025-03-14 22:42:13,732 - distributed.worker - ERROR - Compute Failed
Key:       ('concatenate-mean_chunk-2095ee7f44f2369d1574a963dd551b57', 20, 0, 0)
State:     executing
Task:  <Task ('concatenate-mean_chunk-2095ee7f44f2369d1574a963dd551b57', 20, 0, 0) _execute_subgraph(...)>
Exception: "OSError(-101, 'NetCDF: HDF error')"
Traceback: '  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/dask/array/core.py", line 133, in getter\n    c = np.asarray(c)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 578, in __array__\n    return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 583, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 794, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 664, in get_duck_array\n    array = array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 657, in get_duck_array\n    array = self.array[self.key]\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 103, in __getitem__\n    return indexing.explicit_indexing_adapter(\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 1018, in explicit_indexing_adapter\n    result = raw_indexing_method(raw_key.tuple)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 115, in _getitem\n    original_array = self.get_array(needs_lock=False)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 94, in get_array\n    ds = self.datastore._acquire(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 455, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/contextlib.py", line 135, in __enter__\n    return next(self.gen)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n  File "src/netCDF4/_netCDF4.pyx", line 2521, in netCDF4._netCDF4.Dataset.__init__\n  File "src/netCDF4/_netCDF4.pyx", line 2158, in netCDF4._netCDF4._ensure_nc_success\n'

2025-03-14 22:42:21,449 - distributed.worker - ERROR - Compute Failed
Key:       ('concatenate-mean_chunk-4cb7f6e34188a10cefe2cef465c93dc4', 9, 0, 0)
State:     executing
Task:  <Task ('concatenate-mean_chunk-4cb7f6e34188a10cefe2cef465c93dc4', 9, 0, 0) _execute_subgraph(...)>
Exception: 'RuntimeError("NetCDF: Can\'t open HDF5 attribute")'
Traceback: '  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/dask/array/core.py", line 133, in getter\n    c = np.asarray(c)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 578, in __array__\n    return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 583, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 794, in get_duck_array\n    return self.array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 664, in get_duck_array\n    array = array.get_duck_array()\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/coding/variables.py", line 81, in get_duck_array\n    return self.func(self.array.get_duck_array())\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 657, in get_duck_array\n    array = self.array[self.key]\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 103, in __getitem__\n    return indexing.explicit_indexing_adapter(\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/core/indexing.py", line 1018, in explicit_indexing_adapter\n    result = raw_indexing_method(raw_key.tuple)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 115, in _getitem\n    original_array = self.get_array(needs_lock=False)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 94, in get_array\n    ds = self.datastore._acquire(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/netCDF4_.py", line 455, in _acquire\n    with self._manager.acquire_context(needs_lock) as root:\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/contextlib.py", line 135, in __enter__\n    return next(self.gen)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 199, in acquire_context\n    file, cached = self._acquire_with_cache_info(needs_lock)\n  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 217, in _acquire_with_cache_info\n    file = self._opener(*self._args, **kwargs)\n  File "src/netCDF4/_netCDF4.pyx", line 2540, in netCDF4._netCDF4.Dataset.__init__\n  File "src/netCDF4/_netCDF4.pyx", line 2055, in netCDF4._netCDF4._get_vars\n  File "src/netCDF4/_netCDF4.pyx", line 2164, in netCDF4._netCDF4._ensure_nc_success\n'

Exception ignored in: <function CachingFileManager.__del__ at 0x14b90596fb50>
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 250, in __del__
    self.close(needs_lock=False)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 234, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2669, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2636, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2164, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: HDF error
2025-03-14 22:42:30,554 - distributed.nanny - INFO - Worker process 4024863 was killed by signal 11
2025-03-14 22:42:30,559 - distributed.nanny - WARNING - Restarting worker
2025-03-14 22:42:31,639 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-f9l6zrd4', purging
2025-03-14 22:42:32,468 - distributed.worker - INFO -       Start worker at:      tcp://10.6.8.13:41807
2025-03-14 22:42:32,468 - distributed.worker - INFO -          Listening to:      tcp://10.6.8.13:41807
2025-03-14 22:42:32,468 - distributed.worker - INFO -           Worker name:             SLURMCluster-6
2025-03-14 22:42:32,468 - distributed.worker - INFO -          dashboard at:            10.6.8.13:38473
2025-03-14 22:42:32,468 - distributed.worker - INFO - Waiting to connect to: tcp://146.186.150.11:36245
2025-03-14 22:42:32,468 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:32,468 - distributed.worker - INFO -               Threads:                          1
2025-03-14 22:42:32,468 - distributed.worker - INFO -                Memory:                   4.00 GiB
2025-03-14 22:42:32,468 - distributed.worker - INFO -       Local Directory: /scratch/cta5244/dask-workers/dask-scratch-space/worker-gk8d_acs
2025-03-14 22:42:32,468 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:33,117 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-14 22:42:33,117 - distributed.worker - INFO -         Registered to: tcp://146.186.150.11:36245
2025-03-14 22:42:33,117 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:33,118 - distributed.core - INFO - Starting established connection to tcp://146.186.150.11:36245
2025-03-14 22:42:36,075 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.8.33:46453
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2075, in gather_dep
    response = await get_data_from_worker(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/worker.py", line 2881, in get_data_from_worker
    response = await send_recv(
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.8.13:56352 remote=tcp://10.6.8.33:46453>: ConnectionResetError: [Errno 104] Connection reset by peer
Exception ignored in: <function CachingFileManager.__del__ at 0x14f97e95d2d0>
Traceback (most recent call last):
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 250, in __del__
    self.close(needs_lock=False)
  File "/storage/home/cta5244/mambaforge/envs/pyWBM/lib/python3.10/site-packages/xarray/backends/file_manager.py", line 234, in close
    file.close()
  File "src/netCDF4/_netCDF4.pyx", line 2669, in netCDF4._netCDF4.Dataset.close
  File "src/netCDF4/_netCDF4.pyx", line 2636, in netCDF4._netCDF4.Dataset._close
  File "src/netCDF4/_netCDF4.pyx", line 2164, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: HDF error
2025-03-14 22:42:41,589 - distributed.nanny - INFO - Worker process 4025811 was killed by signal 11
2025-03-14 22:42:41,592 - distributed.nanny - WARNING - Restarting worker
2025-03-14 22:42:42,643 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cta5244/dask-workers/dask-scratch-space/worker-4uczsfs5', purging
2025-03-14 22:42:43,369 - distributed.worker - INFO -       Start worker at:      tcp://10.6.8.13:38925
2025-03-14 22:42:43,369 - distributed.worker - INFO -          Listening to:      tcp://10.6.8.13:38925
2025-03-14 22:42:43,370 - distributed.worker - INFO -           Worker name:             SLURMCluster-6
2025-03-14 22:42:43,370 - distributed.worker - INFO -          dashboard at:            10.6.8.13:46861
2025-03-14 22:42:43,370 - distributed.worker - INFO - Waiting to connect to: tcp://146.186.150.11:36245
2025-03-14 22:42:43,370 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:43,370 - distributed.worker - INFO -               Threads:                          1
2025-03-14 22:42:43,370 - distributed.worker - INFO -                Memory:                   4.00 GiB
2025-03-14 22:42:43,370 - distributed.worker - INFO -       Local Directory: /scratch/cta5244/dask-workers/dask-scratch-space/worker-i0sf78m3
2025-03-14 22:42:43,370 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:43,811 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-14 22:42:43,812 - distributed.worker - INFO -         Registered to: tcp://146.186.150.11:36245
2025-03-14 22:42:43,812 - distributed.worker - INFO - -------------------------------------------------
2025-03-14 22:42:43,812 - distributed.core - INFO - Starting established connection to tcp://146.186.150.11:36245
slurmstepd: error: *** JOB 35099575 ON p-bc-5003 CANCELLED AT 2025-03-14T22:42:44 ***
